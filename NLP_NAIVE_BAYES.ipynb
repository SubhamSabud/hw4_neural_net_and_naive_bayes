{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c613a17-397e-49f7-a573-e0acdd23b06e",
   "metadata": {},
   "source": [
    "## calculating the priori & conditionals probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497261c1-5cc5-4822-baf8-5586b37c4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities:\n",
      "e = 0.3333\n",
      "j = 0.3333\n",
      "s = 0.3333\n",
      "Conditional probabilities for English:\n",
      "[0.06765523632993511, 0.007599629286376275, 0.01723818350324375, 0.022428174235403147, 0.10620945319740499, 0.016867469879518066, 0.009823911028730302, 0.05690454124189064, 0.05060240963855422, 0.0005560704355885081, 0.0009267840593141799, 0.03021316033364226, 0.020203892493049123, 0.05430954587581093, 0.05542168674698795, 0.01723818350324375, 0.0005560704355885081, 0.045041705282669146, 0.06728452270620947, 0.09249304911955515, 0.025023169601482858, 0.008341056533827617, 0.013901760889712692, 0.002038924930491196, 0.016496756255792396, 0.0005560704355885081, 0.19406858202038926]\n",
      "Conditional probabilities for Spanish:\n",
      "[0.09113817723645527, 0.011339773204535913, 0.026459470810583793, 0.04157916841663167, 0.10625787484250314, 0.008819823603527928, 0.007139857202855945, 0.007979840403191937, 0.0499790004199916, 0.004619907601847965, 0.002099958000839984, 0.046619067618647626, 0.015539689206215871, 0.05501889962200755, 0.09197816043679126, 0.026459470810583793, 0.0130197396052079, 0.06761864762704746, 0.05417891642167157, 0.029819403611927756, 0.040739185216295666, 0.004619907601847965, 0.0004199916001679967, 0.0037799244015119695, 0.009659806803863925, 0.007979840403191937, 0.17513649727005462]\n",
      "Conditional probabilities for Japanese:\n",
      "[0.13289349670122524, 0.0028275212064090473, 0.0028275212064090473, 0.016022620169651277, 0.04995287464655985, 0.0009425070688030163, 0.016022620169651277, 0.025447690857681428, 0.10273327049952875, 0.0009425070688030163, 0.051837888784165884, 0.0009425070688030163, 0.03675777568331763, 0.06126295947219604, 0.07822808671065035, 0.0009425070688030163, 0.0009425070688030163, 0.023562676720075403, 0.05372290292177191, 0.06314797360980207, 0.08199811498586237, 0.0009425070688030163, 0.034872761545711596, 0.0009425070688030163, 0.012252591894439207, 0.017907634307257305, 0.1291234684260132]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import the necessary modules\n",
    "import os\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Define the vocabulary size and the smoothing parameter\n",
    "V = 27 # 26 letters and space\n",
    "alpha = 0.5 # additive smoothing with parameter 1/2\n",
    "\n",
    "# Define the class labels and the data directories\n",
    "classes = ['e', 'j', 's'] # English, Japanese, and Spanish\n",
    "data_dir = r'C:\\Users\\Subham Sabud\\OneDrive\\Desktop\\python ece760\\hw4_ece760_uw_madison\\languageID'\n",
    "#data_dir = 'C:\\\\Users\\\\Subham Sabud\\\\OneDrive\\\\Desktop\\\\python ece760hw4_ece760_uw_madison\\\\languageID\\\\languageID' # the directory where the data files are stored\n",
    "\n",
    "# Define a function to preprocess a text file\n",
    "def preprocess(file):\n",
    "    # Open the file and read its content\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.readline().rstrip(\"\\n\")\n",
    "    \n",
    "    # Convert all characters to lower case and remove non-printable characters\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c in string.printable)\n",
    "    \n",
    "    # Count the frequency of each character and return a counter object\n",
    "    counter = collections.Counter(text)\n",
    "    return counter\n",
    "\n",
    "# Define a function to split the data into training and test sets\n",
    "def split_data(data_dir, classes):\n",
    "    # Initialize empty lists to store the training and test data\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # Loop through each class label and each data file\n",
    "    for c in classes:\n",
    "        # Get the list of files in the subdirectory for that class\n",
    "        files = os.listdir(os.path.join(data_dir, c))\n",
    "        \n",
    "        # Loop through the first 10 files (0.txt to 9.txt) and append them to the training data list\n",
    "        for i in range(10):\n",
    "            file = os.path.join(data_dir, c, files[i])\n",
    "            train_data.append((preprocess(file), c))\n",
    "        \n",
    "        # Loop through the remaining 10 files (10.txt to 19.txt) and append them to the test data list\n",
    "        for i in range(10, 20):\n",
    "            file = os.path.join(data_dir, c, files[i])\n",
    "            test_data.append((preprocess(file), c))\n",
    "    \n",
    "    # Return the training and test data lists\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define a function to estimate the prior probabilities for each class\n",
    "def estimate_priors(train_data, classes, alpha):\n",
    "    # Initialize an empty dictionary to store the prior probabilities\n",
    "    priors = {}\n",
    "    \n",
    "    # Get the total number of documents in the training data\n",
    "    N = len(train_data)\n",
    "    \n",
    "    # Get the number of classes\n",
    "    C = len(classes)\n",
    "    \n",
    "    # Loop through each class label\n",
    "    for c in classes:\n",
    "        # Count the number of documents in that class\n",
    "        N_c = sum(1 for x, y in train_data if y == c)\n",
    "        \n",
    "        # Apply additive smoothing and calculate the logarithm of the prior probability\n",
    "        priors[c] = math.log((N_c + alpha) / (N + alpha * C))\n",
    "        #priors[c] = ((N_c + alpha) / (N + alpha * C))\n",
    "    # Return the prior probabilities dictionary\n",
    "    return priors\n",
    "\n",
    "\n",
    "# Define a function to estimate the conditional probabilities for each character given each class\n",
    "def estimate_conditionals(train_data, classes, V, alpha):\n",
    "    # Initialize an empty dictionary to store the conditional probabilities\n",
    "    conditionals = {}\n",
    "    \n",
    "    # Loop through each class label\n",
    "    for c in classes:\n",
    "        # Initialize an empty dictionary to store the conditional probabilities for that class\n",
    "        conditionals[c] = {}\n",
    "        \n",
    "        # Get the total number of characters in documents of that class\n",
    "        n_c = sum(sum(x.values()) for x, y in train_data if y == c)\n",
    "        #n_c = sum(1 for x, y in train_data if y == c)\n",
    "        # Loop through each character in the vocabulary\n",
    "        for x_i in string.ascii_lowercase + ' ' :\n",
    "            # Count the number of times that character appears in documents of that class\n",
    "            n_ic = sum(x[x_i] for x, y in train_data if y == c)\n",
    "            \n",
    "            # Apply additive smoothing and calculate the logarithm of the conditional probability\n",
    "            conditionals[c][x_i] = math.log((n_ic + alpha) / (n_c + alpha * V))\n",
    "    \n",
    "    # Return the conditional probabilities dictionary\n",
    "    return conditionals\n",
    "\n",
    "# Define a function to classify a test document using the Naive Bayes rule\n",
    "def classify(test_doc, priors, conditionals, classes):\n",
    "    # Initialize an empty dictionary to store the scores for each class\n",
    "    scores = {}\n",
    "    \n",
    "    # Loop through each class label\n",
    "    for c in classes:\n",
    "        # Initialize the score with the prior probability of that class\n",
    "        scores[c] = 0\n",
    "        \n",
    "        # Loop through each character in the test document\n",
    "        for x_i in test_doc.keys():\n",
    "            # Add the logarithm of the conditional probability of that character given that class to the score\n",
    "            scores[c] += priors[c]+ conditionals[c][x_i] * test_doc[x_i]\n",
    "    \n",
    "    # Find the class that maximizes the score\n",
    "    pred_class = max(scores, key=scores.get)\n",
    "    \n",
    "    # Return the predicted class label\n",
    "    return pred_class\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = split_data(data_dir, classes)\n",
    "\n",
    "# Estimate the prior probabilities for each class\n",
    "priors = estimate_priors(train_data, classes, alpha)\n",
    "\n",
    "# Estimate the conditional probabilities for each character given each class\n",
    "conditionals = estimate_conditionals(train_data, classes, V, alpha)\n",
    "\n",
    "# Initialize an empty list to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Loop through each test document and its true class label\n",
    "for test_doc, true_class in test_data:\n",
    "    # Classify the test document using the Naive Bayes rule\n",
    "    pred_class = classify(test_doc, priors, conditionals, classes)\n",
    "    \n",
    "    # Append the predicted class label to the predictions list\n",
    "    predictions.append(pred_class)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = sum(1 for i in range(len(test_data)) if predictions[i] == test_data[i][1]) / len(test_data)\n",
    "\n",
    "# Print the accuracy\n",
    "#print(\"Accuracy of the character-based Naive Bayes classifier =\", round(accuracy, 2))\n",
    "\n",
    "# Print the prior probabilities\n",
    "print(\"Prior probabilities:\")\n",
    "for c in classes:\n",
    "    print(c, \"=\", round(math.exp(priors[c]), 4))\n",
    "\n",
    "# Print the conditional probabilities for English\n",
    "print(\"Conditional probabilities for English:\")\n",
    "theta_e = []\n",
    "for x_i in string.ascii_lowercase + ' ':\n",
    "    theta_e.append(math.exp(conditionals['e'][x_i]))\n",
    "print(theta_e)\n",
    "\n",
    "# Print the conditional probabilities for Spanish\n",
    "print(\"Conditional probabilities for Spanish:\")\n",
    "theta_s = []\n",
    "for x_i in string.ascii_lowercase + ' ':\n",
    "    theta_s.append(math.exp(conditionals['s'][x_i]))\n",
    "print(theta_s)\n",
    "\n",
    "# Print the conditional probabilities for Japanese\n",
    "print(\"Conditional probabilities for Japanese:\")\n",
    "theta_j = []\n",
    "for x_i in string.ascii_lowercase + ' ':\n",
    "    theta_j.append(math.exp(conditionals['j'][x_i]))\n",
    "print(theta_j)\n",
    "\n",
    "#file = \"e10.txt\"\n",
    "#test_doc=file\n",
    "# Classify the test document using the Naive Bayes rule\n",
    "#pred_class = classify(test_doc, priors, conditionals, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9397f83d-285a-41eb-a3b9-1b31f2c6d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({' ': 107, 'e': 69, 't': 52, 's': 44, 'a': 34, 'o': 33, 'h': 32, 'n': 30, 'r': 28, 'i': 26, 'p': 14, 'w': 12, 'm': 12, 'd': 10, 'l': 10, 'u': 10, 'f': 10, 'c': 9, 'v': 8, 'y': 7, 'g': 6, 'x': 2, 'b': 2, 'q': 1, 'z': 1})\n",
      "The bag-of-words count vector x for e10.txt is:\n",
      "[ 34.   2.   9.  10.  69.  10.   6.  32.  26.   0.   0.  10.  12.  30.\n",
      "  33.  14.   1.  28.  44.  52.  10.   8.  12.   2.   7.   1. 107.]\n"
     ]
    }
   ],
   "source": [
    "# Define the file name\n",
    "file = \"e10.txt\"\n",
    "\n",
    "# Define a function to preprocess a text file\n",
    "def preprocess(file):\n",
    "    # Open the file and read its content\n",
    "    with open(file, \"r\") as f:\n",
    "       text = f.readline().rstrip(\"\\n\")\n",
    "    \n",
    "    # Convert all characters to lower case and remove non-printable characters\n",
    "    text = text.lower()\n",
    "    text = \"\".join(c for c in text if c in string.printable)\n",
    "    \n",
    "    # Count the frequency of each character and return a counter object\n",
    "    counter = collections.Counter(text)\n",
    "    return counter\n",
    "\n",
    "# Preprocess the text file and get the counter object\n",
    "counter = preprocess(file)\n",
    "\n",
    "# Print the counter object\n",
    "print(counter)\n",
    "\n",
    "\n",
    "\n",
    "#To create the bag-of-words vector x for e10.txt, we need to initialize a vector of length 27 with all zeros \n",
    "#and then assign the values from the counter object according to the vocabulary order. Here is a possible code that does that:\n",
    "\n",
    "# Import numpy to create a vector\n",
    "import numpy as np\n",
    "\n",
    "# Define the vocabulary of 27 characters\n",
    "vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
    "# Initialize a vector of length 27 with all zeros\n",
    "x = np.zeros(27)\n",
    "\n",
    "# Loop through each character in the vocabulary\n",
    "for i, c in enumerate(vocabulary):\n",
    "    # Assign the value from the counter object to the corresponding position in the vector\n",
    "    x[i] = counter[c]\n",
    "\n",
    "# Print the vector x\n",
    "print(\"The bag-of-words count vector x for e10.txt is:\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c25217-9a1f-4033-8c5c-162679e59d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loglikelihood probability for English is:\n",
      "-1577.3264186291008\n",
      "The loglikelihood probability for Japanese is:\n",
      "-1800.1283175527014\n",
      "The loglikelihood probability for Spanish is:\n",
      "-1710.1689804601124\n"
     ]
    }
   ],
   "source": [
    "#for c in classes:\n",
    "loglikelihood_e = loglikelihood_j=loglikelihood_s=0\n",
    "for i in range (len(x)):\n",
    "    loglikelihood_e += x[i]* math.log(theta_e[i]);\n",
    "    loglikelihood_j += x[i]* math.log(theta_j[i]);\n",
    "    loglikelihood_s += x[i]* math.log(theta_s[i]);\n",
    "\n",
    "\n",
    "print(\"The loglikelihood probability for English is:\")\n",
    "print(loglikelihood_e)\n",
    "print(\"The loglikelihood probability for Japanese is:\")\n",
    "print(loglikelihood_j)\n",
    "print(\"The loglikelihood probability for Spanish is:\")\n",
    "print(loglikelihood_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e9de6702-2d22-4e2e-bfd4-d21acd1203a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      e  j  s\n",
      "Predicted          \n",
      "e          10  4  5\n",
      "j           0  6  0\n",
      "s           0  0  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "    \n",
    " \n",
    "   \n",
    "  \n",
    "\n",
    "# Import pandas to create a confusion matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Create a data frame with the true and predicted class labels\n",
    "df = pd.DataFrame({'y_actual': [y for x, y in test_data], 'y_predicted': predictions})\n",
    "\n",
    "# Create a confusion matrix using pandas crosstab function\n",
    "confusion_matrix = pd.crosstab(df['y_predicted'], df['y_actual'], rownames=['Predicted'], colnames=['actual'])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
